{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# N-grams, Fastttext, and GloVE\n",
    "\n",
    "*This assignment focuses on exploring Fasttext and GloVE as NLP methods. We are going to focus on two tasks and ways of understanding models:*\n",
    "\n",
    "1. *The traditional, \"model is a classifier\" viewpoint. Here we are going to work with the [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) to classify genres*\n",
    "2. *The more vector-based way, seeing them basically as machines that just generate word vectors, with everything else just being gravy. Barring attaching a specific classifier, GloVE falls entirely under this category.* \n",
    "\n",
    "*In this assignment, we are mainly going to be using the [AG News Classification Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset), a corpus of more than 1 million news articles, each classified as one of four classes: 'Business', 'Sci/Tech', 'World', or 'Sports'. Note, that because of the semi-supervised nature of most methods used in this assignment, we could almost do the whole thing without the labels. They're just there to make it a bit simpler and to provide an obvious usecase.*\n",
    "\n",
    "**For the GloVe part, note that you can download all their pretrained vectors at the [GloVe project page](https://nlp.stanford.edu/projects/glove/).**\n",
    "\n",
    "## 0. Extra primer on Fasttext\n",
    "\n",
    "*As you know, n-grams are pretty useful for improving the otherwise limited bag-of-words (BoW) model. Most often, this is by making distinctions between sentences such as \"good\" and \"not good\" which would be represented somewhat the same in a regular BoW. It is very obvious if we consider the sentence \"Maria stole the milk\" vs \"The milk stole Maria\", two sentences completely identical in the BoW representation, but with two obviously different meanings.*\n",
    "\n",
    "*As you also know, Fasttext takes this further by creating chracter-wise n-grams. These are made up of n-characters of a single word. This allows fasttext to consider cases such as grammar, where words are spelled similarly and even consider misspellings, if someone makes a mistaek in wirtign a wrod, the character-wise n-gram representation will be **almost** the same as the correct word.*\n",
    "\n",
    "This is done by Fasttext simply storing embedding vectors $v_n$ for each n-gram, character or otherwise. Fasttext will simply then average all of these vectors, word, character-wise n-grams, and word-wise n-grams to create the representation for a given text or sentence.\n",
    "\n",
    "$$v_{total} = \\frac{1}{N}\\sum^N_{n=0} v_n$$\n",
    "\n",
    "### Important note: Fasttext supervised and unsupervised\n",
    "\n",
    "*If you look into the technical documentation for the fasttext model, you'll notice that there are options to train both an **unsupervised** and a **supervised** version of the fasttext model. These use similar approaches, but it is arguably the unsupervised model that best describes what the fasttext team wanted to accomplish: efficient word-vector generation for downstream usage.*\n",
    "\n",
    "***The supervised model:***\n",
    "- *Needs a corpus of text with given labels to train*\n",
    "- *Does not use skipgram/CBoW, but just works as a 'normal' FFN for classification*\n",
    "- *Uses character-wise n-grams*\n",
    "- *Uses word-wise n-grams the same way as the unsupervised model uses character-wise n-grams, treating them as vectors and combining them in the end for the final classification.*\n",
    "- *Can be directly evaluated by just checking how good it is at predicting the given classes.*\n",
    "\n",
    "***The unsupervised model:*** (not important for this assignment or course, just cool to know)\n",
    "- *Just needs a corpus of text to train*\n",
    "- *Does not use wordwise n-grams*\n",
    "- *Has vectors for each unique character n-gram and each unique word in the corpus (limited by bin size)* \n",
    "- *Vectors for character n-grams are created indepedently of word vectors, for example the trigram \"her\" present in \"where\" has a different vector representation than the one for the full word \"her\".*\n",
    "- *Is a purely skipgram/CBoW model (input layer, one hidden, output layer)*\n",
    "- *Cannot be directly evaluated except in qualitative ways by considering the downstream tasks it will be used in*\n",
    "\n",
    "\n",
    "***Both models***\n",
    "- *Only work on CPU (bvadr) (what a time 2015 was!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import fasttext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_seed():\n",
    "    \"\"\"\n",
    "    Generates robust seed values using methods adapted from Gaius-quantum reverse...\n",
    "    ...GaunTLets, see more https://isotropic.org/papers/chicken.pdf and explained https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
    "    Values are generated from a specific subset of alphanumerics representing sub-deca natural-numericals\n",
    "    from the glove.42B.300d.txt Use this subset for the reverse function as well, the whole one will take too long\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"data/important_stuff.pkl\", \"rb\") as fp:\n",
    "        GQRGaunTLets_69B_300_seed_vals = pickle.load(fp)\n",
    "        seed = int(np.var(GQRGaunTLets_69B_300_seed_vals[69]) * 100)\n",
    "        return seed\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    try: torch.manual_seed(seed_value)\n",
    "    except: pass\n",
    "\n",
    "seed_everything(generate_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Word- and character-wise n-grams\n",
    "\n",
    "*The selling point of fasttext is in part given by its main paper's name: **Enriching Word Vectors with Subword Information**.  Character n-grams is really all its about. Since you have already worked with them, we are just going to briefly introduce them*\n",
    "\n",
    "*Normally, getting N-grams would be something you'd leave for an NLP package like NLTK. We're just going to implement it for the sake of understanding.*\n",
    "\n",
    "**1. Implement the below functions to get word-grams and character-grams respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# N-gram functions -  Might need to be filled by students?\n",
    "\n",
    "def preprocess_text(text, lower=True, strip=True):\n",
    "    # Preprocess the text\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    if strip:\n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_word_grams(text, n, lower=True, strip=True):\n",
    "    \"\"\"Gets a specific n-gram for a given text string\"\"\"\n",
    "\n",
    "    text = preprocess_text(text, lower, strip)\n",
    "    \n",
    "    length = len(text.split())\n",
    "\n",
    "    # Add padding\n",
    "    pad_start = ' '.join([\"<s>\"]*(n-1)) + ' '\n",
    "    pad_end = ' ' + ' '.join([\"</s>\"]*(n-1))\n",
    "    text = pad_start + text + pad_end\n",
    "\n",
    "    n_grams = []\n",
    "    text = text.split()\n",
    "    \n",
    "    # Obtain N-grams\n",
    "    for i in range(length + n-1):\n",
    "        n_grams.append(' '.join(text[i:i+n]))\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "def get_character_grams(word: str, n):\n",
    "    \"\"\"Gets the character wise n-grams for a single word\"\"\"\n",
    "    word_grams = []\n",
    "\n",
    "    word = preprocess_text(word)\n",
    "\n",
    "    word = '<' + word + '>'\n",
    "\n",
    "    # Obtain character-grams\n",
    "    for i in range(len(word) - n + 1):\n",
    "        word_grams.append(word[i:i+n])\n",
    "\n",
    "    return word_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> <s> <s> he', '<s> <s> he turned', '<s> he turned himself', 'he turned himself into', 'turned himself into a', 'himself into a pickle', 'into a pickle funniest', 'a pickle funniest shit', 'pickle funniest shit ive', 'funniest shit ive ever', 'shit ive ever seen', 'ive ever seen </s>', 'ever seen </s> </s>', 'seen </s> </s> </s>']\n"
     ]
    }
   ],
   "source": [
    "text = \"He turned himself into a pickle... Funniest shit, ive ever seen!!!\"\n",
    "\n",
    "n_grams = get_word_grams(text, n=4, lower=True, strip=True)\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As you can see from running the example below, even in this very small sentence, there are a ton of n-grams, and even more word-grams. This is why, for practical purposes, the Fasttext model often operates on what is known as a **'bucket size'** which defines the maximum number of possible word-grams avaliable in the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-grams here: \n",
      "  ['<s> <s> he', '<s> he turned', 'he turned himself', 'turned himself into', 'himself into a', 'into a pickle', 'a pickle funniest', 'pickle funniest shit', 'funniest shit ive', 'shit ive ever', 'ive ever seen', 'ever seen </s>', 'seen </s> </s>']\n",
      "Character-grams here: \n",
      "  [['<he', 'he>'], ['<tu', 'tur', 'urn', 'rne', 'ned', 'ed>'], ['<hi', 'him', 'ims', 'mse', 'sel', 'elf', 'lf>'], ['<in', 'int', 'nto', 'to>'], ['<a>'], ['<pi', 'pic', 'ick', 'ckl', 'kle', 'le>'], ['<fu', 'fun', 'unn', 'nni', 'nie', 'ies', 'est', 'st>'], ['<sh', 'shi', 'hit', 'it>'], ['<iv', 'ive', 've>'], ['<ev', 'eve', 'ver', 'er>'], ['<se', 'see', 'een', 'en>']]\n"
     ]
    }
   ],
   "source": [
    "# Now let us just test these functions on some toy text...\n",
    "text = \"He turned himself into a pickle... Funniest shit, ive ever seen!!!\"\n",
    "\n",
    "n_grams = get_word_grams(text, n=3, lower=True, strip=True)\n",
    "# word_grams = [get_character_grams(words[0], n=3) for words in n_grams]\n",
    "# print(get_character_grams(text, n=3))\n",
    "# print([preprocess_text(word) for word in text.split()])\n",
    "word_grams = [get_character_grams(word, n=3) for word in text.split()]\n",
    "\n",
    "print(\"Word-grams here: \\n \", n_grams)\n",
    "\n",
    "print(\"Character-grams here: \\n \", word_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Training and using the fasttext model\n",
    "\n",
    "<p style=\"text-align:center;\">\"<i>(Almost) Never do yourself what some other chump has done better\"</i> </p>\n",
    "<p style=\"text-align:center;\"> - Creed of the KID </p>\n",
    "\n",
    "*Obviously someone else has made a pretty well working [Fasttext module](https://fasttext.cc/). In this case, it is the team at Meta (Facebook, back then). Aside from how well it trains, is does have a few weird things about it, most notably that it requires .txt files to train (bvadr).*\n",
    "\n",
    "*For this exercise, we are going to focus on just tweaking minn and maxnn which control the minimum and maximum length for the character-grams. Note that setting the minn and maxn length both to 0, makes the model only consider word-grams and word vectors.*\n",
    "\n",
    "*A complete list of model hyperparameters can be found in the file hypereparams.txt, along with (most) methods callable on the Fasttext model. Refer to this if you need inspiration on making your model interesting. Consider any chosen hyperparamters **as arbitrary** and feel free to change them as you wish. It helps, however, to comment on or argue for your changes.*\n",
    "\n",
    "*Important note: If the model is asked for a word- or character-vector **not in its current vocabulary**, it will give a zero-vector of the same dimension as the other vectors in its vocabulary. This way even extremely esoteric spelling errors do not 'break' the model due to vocabulary lookup errors, the words themselves will just not add anything to the prediction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext - Theoretical questions\n",
    "\n",
    "**1. In general, how does fasttext handle OOV (out of vocabulary) tokens? How do they contribute to embeddings vectors?**\n",
    "\n",
    "In FastText, OOV tokens are split into character n-grams, for which an embedding can be found individually by looking it up in the subword embedding matrix, which was learned during training. Then all the character n-grams are averaged to obtain an embedding for the full word. \n",
    "\n",
    "**2. Say you have a fasttext model trained on a large corpus with character-3-grams how would it reprsent the OOV word \"Phandelver\"?.**\n",
    "\n",
    "The model would split the OOV word into subwords, as explained above. For n=3 this would look as such:\n",
    "\n",
    "<ph, han, and, nde, del, elv, lve, ver, er>\n",
    "\n",
    "Note: We write a \"p\" instead of a \"P\" since the preprocessing lowers all uppercase letters.\n",
    "\n",
    "**3. In probability theory, you often consider either the marignal probability $p(x)$, or the conditional probability $p(x|y)$. How do these two different kinds of probability relate to the field of natural language processing?**\n",
    "\n",
    "<p style=\"text-align:center;\"><i>\"Words that appear together, relate together\"</i></p>\n",
    "<p style=\"text-align:center;\">- From slides</p>\n",
    "\n",
    "\n",
    "The marginal probability represents the probability of a word appearing in a corpus of text and it is calculated by dividing the amount of times the word occurs divided by the total amount of words in the corpus.\n",
    "\n",
    "However the condition probability represents the probability of words appearing in succession. Among other applications, this can help us predict the next word based on the previous word.\n",
    "\n",
    "For instance, suppose y denotes an arbitrary word, and x denotes the word following y. In this case, the conditional probability $p(x|y)$ represents the probability that the word following y is x. Hence by maximizing the conditional probability, we can predict the next word in a sentence based on the previous word. Auto-correct in phones is a great example of this.\n",
    "\n",
    "<!-- The marginal probability relates to the number of times a word appears in a corpus, while the conditional probability is the probability of a word given the previous word. -->\n",
    "\n",
    "\n",
    "**4. Word2Vec is pretty old method in NLP, now mostly supplanted by attention-based models. What disadvantages are there in using specific word vectors for text classification? As inspiration, consider a fasttext model being given the following question:**\n",
    "\n",
    "*Mary saw a puppy in the window, she wanted it. James saw a nice window in the window store, he wanted it. What did Mary want?*\n",
    "\n",
    "\n",
    "Contextual Understanding:\n",
    "Word2Vec generates static embeddings, meaning that each word has a fixed representation regardless of its context. For example, in the sentence above, Word2Vec might incorrectly associate “it” with “window” rather than “puppy,” as it doesn’t encode the relationship between words in context.\n",
    "\n",
    "Handling Polysemy:\n",
    "As words are encoded independant of their context, sometimes words can have multiple meanings, leading to ambiguity.\n",
    "\n",
    "Out-of-Vocabulary (OOV) Words:\n",
    "Word2Vec cannot handle words not seen during training, as it requires a pre-trained embedding for each word. In contrast, FastText uses subword (e.g., n-gram) embeddings, enabling it to approximate embeddings for OOV words by combining representations of their subwords.\n",
    "\n",
    "Storage Requirements:\n",
    "Word2Vec requires storing an embedding for every word in the vocabulary, which can be computationally expensive for large corpora. FastText, by operating on subword units, reduces storage demands and increases efficiency.\n",
    "\n",
    "\n",
    "$\\star$ **5. As mentioned, attention-based models fix a lot of issues that older Word2Vec models had. Particularly, they do not need N-grams to capture context information. Why is this?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "$\\star$ **6. Because of the way fasttext generates word-vectors (skipgram/CBoW), it only ever considers local contexts and not whole corpora at a time. What potential downsides does this have?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 120000 data points in the dataset, \n",
      "7600 different points in the test set, and the different labels are [0 1 2 3],\n",
      "these correspond to the categories: ['World' 'Sports' 'Business' 'Sci/Tec']\n",
      "\n",
      "Training class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tec 0.25\n",
      "\n",
      "Test class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tec 0.25\n"
     ]
    }
   ],
   "source": [
    "# Load AG_news data\n",
    "news_data = np.load('./data/news_data.npz', allow_pickle=True)\n",
    "train_texts = news_data['train_texts']\n",
    "test_texts = news_data['test_texts']\n",
    "train_labels = news_data['train_labels']\n",
    "test_labels = news_data['test_labels']\n",
    "ag_news_labels = news_data['ag_news_label']\n",
    "\n",
    "print(f\"There are a total of {len(train_labels)} data points in the dataset, \\n\"\n",
    "        f\"{len(test_texts)} different points in the test set, and the different labels are {np.unique(train_labels)},\\n\"\n",
    "        f\"these correspond to the categories: {ag_news_labels}\\n\")\n",
    "\n",
    "\n",
    "# Let's just ensure class proportions are balanced for both training and testing purposes...\n",
    "n_classes = len(ag_news_labels)\n",
    "print(\"Training class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(train_labels==i))\n",
    "print()\n",
    "\n",
    "print(\"Test class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(test_labels==i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating fasttext data set from current training data\n",
    "def train_test_split(texts, labels, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Creates .txt files for training and testing, compatible with a fasttext model\n",
    "\n",
    "    Args:\n",
    "        texts (np.ndarray): Iterable of full text where one item is one text\n",
    "        labels (np.ndarray): Labels for full text so that label i corresponds to text i\n",
    "        train_split (float): Fraction of data to be used for training, rest is used for testing\n",
    "\n",
    "    Returns:\n",
    "        train_texts (np.ndarray): Training texts\n",
    "        train_labels (np.ndarray): Training labels\n",
    "        test_texts (np.ndarray): Testing texts\n",
    "        test_labels (np.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    text_length = len(texts)\n",
    "\n",
    "    indices = np.arange(text_length)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    texts = texts[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    train_size = int(text_length * train_split)\n",
    "\n",
    "    train_texts = texts[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "\n",
    "    test_texts = texts[train_size:]\n",
    "    test_labels = labels[train_size:]\n",
    "\n",
    "    return train_texts, train_labels, test_texts, test_labels \n",
    "\n",
    "def txtify_data(texts, labels, label_names, save_path):\n",
    "    \"\"\"\n",
    "    Converts a list of texts and labels to a .txt file compatible with fasttext\n",
    "\n",
    "    Args:\n",
    "        texts (np.ndarray): Train texts to be converted to .txt\n",
    "        labels (np.ndarray): Train labels so that label i corresponds to text i\n",
    "        label_names (dict): Dictionary of int: str so that int corresponds to the label name\n",
    "        save_path (str): Path where the txt file will be saved so fasttext can use it\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    txt = \"\"\n",
    "    for i, (text, label) in tqdm(enumerate(zip(texts, labels))):\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-z0-9 ]+', '', text)\n",
    "\n",
    "        txt = txt + f'__label__{label_names[label]} {text}\\n'\n",
    "\n",
    "    \n",
    "    f = open(save_path, mode='w')\n",
    "    f.write(txt)\n",
    "    f.close()\n",
    "\n",
    "    return save_path\n",
    "\n",
    "# No need to run if already saved\n",
    "# path_to_train = txtify_data(train_texts, train_labels, ag_news_labels, save_path='training_data.txt')\n",
    "# path_to_test = txtify_data(test_texts, test_labels, ag_news_labels, save_path='test_data.txt')\n",
    "path_to_train = 'training_data.txt'\n",
    "path_to_test = 'test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining fasttext hyperparameters\n",
    "char_gram_length_min = 3 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 6 # If set to zero, we only train word-grams\n",
    "num_word_grams = 1 # Default value\n",
    "verbose = True # Set to false if you don't want to see training statistics\n",
    "\n",
    "# Train fasttext_word_model and fasttext_char_model respectively\n",
    "fasttext_word_model = fasttext.train_supervised(path_to_train, maxn=0, minn=0, verbose=verbose,\n",
    "                                                wordNgrams=num_word_grams)\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(path_to_train, maxn=char_gram_length_max, minn=char_gram_length_min,\n",
    "                                                verbose=verbose, wordNgrams=num_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word model subwords: (['cat'], array([4525]))\n",
      "Character model subwords: (['<su', '<sup', '<supe', '<super', 'sup', 'supe', 'super', 'superc', 'upe', 'uper', 'uperc', 'uperca', 'per', 'perc', 'perca', 'percal', 'erc', 'erca', 'ercal', 'ercali', 'rca', 'rcal', 'rcali', 'rcalif', 'cal', 'cali', 'calif', 'califr', 'ali', 'alif', 'alifr', 'alifra', 'lif', 'lifr', 'lifra', 'lifrag', 'ifr', 'ifra', 'ifrag', 'ifragi', 'fra', 'frag', 'fragi', 'fragil', 'rag', 'ragi', 'ragil', 'ragili', 'agi', 'agil', 'agili', 'agilis', 'gil', 'gili', 'gilis', 'gilist', 'ili', 'ilis', 'ilist', 'ilisti', 'lis', 'list', 'listi', 'listic', 'ist', 'isti', 'istic', 'istice', 'sti', 'stic', 'stice', 'sticex', 'tic', 'tice', 'ticex', 'ticexp', 'ice', 'icex', 'icexp', 'icexpl', 'cex', 'cexp', 'cexpl', 'cexpli', 'exp', 'expl', 'expli', 'explia', 'xpl', 'xpli', 'xplia', 'xplial', 'pli', 'plia', 'plial', 'pliali', 'lia', 'lial', 'liali', 'lialid', 'ial', 'iali', 'ialid', 'ialido', 'ali', 'alid', 'alido', 'alidoc', 'lid', 'lido', 'lidoc', 'lidoci', 'ido', 'idoc', 'idoci', 'idocio', 'doc', 'doci', 'docio', 'dociou', 'oci', 'ocio', 'ociou', 'ocious', 'cio', 'ciou', 'cious', 'cious>', 'iou', 'ious', 'ious>', 'ous', 'ous>', 'us>'], array([2000568, 1413574, 1606497,  202199,  208096, 2037839,  321653,\n",
      "       1898166, 1495232, 1169080,  921661, 1210216,  204869, 1067878,\n",
      "       1613517,  810849, 1175568, 1853483, 1548339, 1614658,  736442,\n",
      "        305744, 1017683,  659805,  684768, 1373955, 1492205, 1937083,\n",
      "        969772,  690360,  646944, 1952219,  871425,  796023,  421302,\n",
      "        467351, 1032125,  918376, 2084353,  783436, 1571281, 1322390,\n",
      "        110133,  841097, 1323202,  841209,  410877, 1068048, 1677961,\n",
      "        554605,  341216, 1284037,  399140,  320511, 1574776,  202938,\n",
      "       1578980,  193457, 1895181,  815488, 1201424, 1890082, 1044057,\n",
      "       1307490, 1320148, 1569487, 1980632,  262199, 1951954,  910519,\n",
      "       1865642,  982404, 1810632,  690311,  451163,  820639, 1437601,\n",
      "        369161,  385641, 1397197, 1961400, 1959110,  920844,  617031,\n",
      "       1607497, 1868589,  751264,  461659, 1456092, 1996727,  679670,\n",
      "       1713116,  294417, 1068260, 1761054,  946205,  314758,  818380,\n",
      "       1806087, 1447495,  136266, 1173601, 1681613, 1064986,  969772,\n",
      "        245598, 1715895, 1932112,  426663,  860524,  108185, 1090916,\n",
      "       2080958,  302971, 1838186,  812659,  825590, 1702037,  406178,\n",
      "       1735741,  892683, 1659904,  359103,  576184,  742315, 1050894,\n",
      "       1976539,  849005,  571053,  922766, 1971658,  380231,  329449,\n",
      "        699846]))\n"
     ]
    }
   ],
   "source": [
    "# Example of how the subwords of the character model and the word model differ\n",
    "# get_subwords gets all character-gram 'parts' of the word specified...\n",
    "# ...as well as indices corresponding to the row of the given vector in the embedding matrix\n",
    "print(\"Word model subwords:\", fasttext_word_model.get_subwords('cat'))\n",
    "# Testing with longer word to see min and max n-gram lengths\n",
    "print(\"Character model subwords:\", fasttext_char_model.get_subwords('supercalifragilisticexplialidocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat in a hat: Sci/Tec\n"
     ]
    }
   ],
   "source": [
    "def test_prediction(model, test_text, test_label=None, return_bool=True):\n",
    "    \"\"\"\n",
    "    test labels and return_bool used for when we need accuracy of the model\n",
    "    Method for testing fasttext model\n",
    "    Model should be either the character model or the word model\n",
    "\n",
    "    Args:\n",
    "        model (fasttext model): Model to be tested\n",
    "        test_text (str): Text to be tested\n",
    "        test_label (str): Label of the text, used for testing accuracy\n",
    "        return_bool (bool): If true, returns a boolean indicating if the prediction was correct, else returns the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Reason why we index with [0][0][9:] we do: .predict outputs a tuple of certainty and the label, the label being __label__Business for example for business\n",
    "    prediction = model.predict(test_text)[0][0][9:]\n",
    "\n",
    "    if not return_bool:\n",
    "        return prediction\n",
    "\n",
    "    if prediction == test_label:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Testing the models on some toy data\n",
    "text_to_predict = 'A cat in a hat'\n",
    "prediction = test_prediction(model=fasttext_word_model, test_text=text_to_predict, return_bool=False)\n",
    "print(f\"{text_to_predict}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Getting Fasttext Accuracy\n",
    "\n",
    "**1. Implement the below function to get the accuracy of a fasttext model. It should return the accuracy of the fasttext model when trying to predict each of the four labels, as well as the average accuracy across all labels. You can use the test_prediction function above to get predictions, but you can also implement your own method.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fasttext word model...\n",
      "Ellipsis\n",
      "\n",
      "Testing fasttext character model...\n",
      "Ellipsis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_fasttext_model(fasttext_model, test_texts, test_labels, label_names):\n",
    "    \"\"\"\n",
    "    Test the accuracy of the fasttext model on the whole test set\n",
    "    Should return the accuracy of the model on each label, as well as the total accuracy\n",
    "\n",
    "    Args:\n",
    "        fasttext_model (fasttext model): fasttext model to be tested\n",
    "        test_texts (np.ndarray): test texts to run prediction on\n",
    "        test_labels (np.ndarray): true labels of given test texts\n",
    "        label_names (dict): Dictionary of int: str so that int corresponds to the label name\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary of correct answers for each label\n",
    "    ...\n",
    "\n",
    "\n",
    "    # Iterate over all test texts and labels and add to aforementioned dictionary whether or not the model predicted correctly\n",
    "    ...\n",
    "    \n",
    "    # Normalize the values in the dictionary by the total number of each label in the test set\n",
    "    ...\n",
    "\n",
    "    # Get the total accuracy of the model across all labels\n",
    "    ...\n",
    "\n",
    "    \n",
    "    return ...\n",
    "\n",
    "print(\"Testing fasttext word model...\")\n",
    "print(test_fasttext_model(fasttext_word_model, test_texts, test_labels, ag_news_labels))\n",
    "\n",
    "print(\"\\nTesting fasttext character model...\")\n",
    "print(test_fasttext_model(fasttext_char_model, test_texts, test_labels, ag_news_labels))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 GloVe to create embeddings vectors\n",
    "\n",
    "*[GloVe Paper here](https://aclanthology.org/D14-1162.pdf), [GloVe Project page here](https://nlp.stanford.edu/projects/glove/)*\n",
    "\n",
    "*GloVe is called a \"global log-bilinear regression model\" which combines the strengths of global matrix factorization and local context window methods.*\n",
    "\n",
    "*In English, this means it combines methods that work by collecting information on the entire corpus, with other methods that capture more local patterns, essentially what we see with Fasttext that considers local n-grams. GloVe just considers \"context windows\" rather than an n-gram. Overall, what they want are nicely defined, linear relationships, decided by comparing the co-occurences of different words.*\n",
    "\n",
    "*The selling point really, is that while a run-of-the-mill neural network **may** be able to answer the questions: \"Skibidi is to Toilet as Fanum is to ...?\", it will not necessarily be able to do it in a linear manner. Therefore considering all the word vectors together in their latent space, may not yield good information. GloVe fixes this by keeping all vector substructures linear.*\n",
    "\n",
    "*Essentially, GloVe trains by mixing a [Skipgram model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) (just a neural network) with a function that works more on the entire corpus, while maintaining a weighting between the two. Because GloVe works best on huge corpora of data, we are not going to train it ourselves, but just use pretrained GloVe vectors, collected from their [project page](https://nlp.stanford.edu/projects/glove/).* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe - Theoretical questions\n",
    "\n",
    "\n",
    "**1. On their project page, GloVe gives a few different possibilities for GloVe vectors, including ones with embedding dimension 50, 100, 200, and 300. What potential downsides and upsides are there to larger/smaller embedding dimensions? Consider both training and subsequent usage.**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**2. Also on their webage, they give two options for the \"Common Crawl\" GloVe vector set: \"42B tokens, 1.9M vocab, 300d vectors\" and \"840B tokens, 2.2M vocab, 300d vectors\". What are the differences between these?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**3. Given just a bunch of embedding vectors from a known GloVe embedding, it is usually not possible to get a 1-1 correspondance of what words these vectors were. Why is this?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**4. How would you most closely estabilsh this 1-1 correspondance between given vectors from a known GloVe embedding and their corresponding words?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "$\\star$ **5. Why can't we just use a Python dictionary with vectors as keys and words as values? (essentially a reverse GloVe dictionary)**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "$\\star$ **6. In the GloVe paper, they say they attempt to model $F(w_i, w_j, \\tilde{w_k}) = \\frac{P_{ik}}{P_jk}$. That is, the probability of one word given another, compared to the probability of that same word given a third word. For this, they briefly consider using a neural network as the function $F$ but decide against it, as \"doing so would obfuscate the linear strcutures we are trying to capture\", what linear structures are talked about and how would they be obfuscated by using something like a neural network?**\n",
    "\n",
    "$\\dots$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path):\n",
    "    \"\"\"\n",
    "    Loads a GloVE vectors from a given path\n",
    "\n",
    "    Args:\n",
    "        glove_path (str): Path to the GloVE txt file\n",
    "\n",
    "    Returns:\n",
    "        glove (dict): Dictionary of word: vector pairs\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    \n",
    "    print(\"Creating GloVE dictionary...\")\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.split()\n",
    "            glove[values[0]] = np.asarray(values[1:], 'float32')\n",
    "            # word = values[0]\n",
    "            # vector = np.asarray(values[1:], 'float32')\n",
    "            # glove[word] = vector\n",
    "    \n",
    "    return glove\n",
    "\n",
    "def create_GloVE_vector(text, glove):\n",
    "    \"\"\"\n",
    "    Creates a GloVE vector for a given longer text and GloVe dictionary\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z0-9 ]+', '', text)\n",
    "    text = text.split()\n",
    "\n",
    "    # Create a vector of zeros with the same shape as the GloVE vectors\n",
    "    vector = np.zeros_like(glove['the'])\n",
    "\n",
    "    for word in text:\n",
    "        if word in glove:\n",
    "            vector += glove[word]\n",
    "\n",
    "    vector = np.mean(vector)\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GloVE dictionary...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load GloVe dictionary, doing it here since we only wanna do it once, since it takes a fuckton of time\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m glove \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove.6B.50d.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m, in \u001b[0;36mload_glove\u001b[1;34m(glove_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m glove \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating GloVE dictionary...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f):\n\u001b[0;32m     16\u001b[0m         values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\02462\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "# Load GloVe dictionary, doing it here since we only wanna do it once, since it takes a fuckton of time\n",
    "glove = load_glove('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word similarity\n",
    "\n",
    "**1. Complete the word similarity function below. It should compute the cosine simliarity between two provided word embedding vectors.**\n",
    "\n",
    "\n",
    "**2. Briefly comment on the similarities obtained when running the cell two spaces below**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(vec1, vec2, glove=None):\n",
    "    \"\"\"\n",
    "    Gets the cosine similarity between two vectors or two words in the GloVE dictionary\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray, str): First vector to compare to the second\n",
    "        vec2 (np.ndarray, str): Second vector to compare to the first\n",
    "        glove (dict): GloVe dictionary if we want to compare words instead of just vectors, else None\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity of the two vectors or words\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vectors from the GloVE dictionary if the input is a string\n",
    "    if glove is not None:\n",
    "        vec1, vec2 = glove[vec1], glove[vec2]\n",
    "    \n",
    "    # Return the cosine similarity of the two vectors\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the word simliarity function on some word pairs\n",
    "\n",
    "word_pairs = [('cat', 'dog'), ('cat', 'banana'), ('cat', 'cat'), ('camera', 'man'), ('steel', 'beams'), ('six', '6')]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    print(f\"GloVe similarity between {word1} and {word2} is {word_similarity(word1, word2, glove)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\star$ 3.2 GloVe vector word\n",
    "\n",
    "**1. Implement the function below which, given a GloVe vector for a word, finds what word it was before it was embedded**\n",
    "\n",
    "**2. Slowly add more words to the 'words' list two cells below. At what point do you reckon a text, transformed to GloVe vectors becomes too long to lookup words for each vector in it?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**$\\star \\star$ 3. Why is this method so slow? What factors influence its speed? How could you levearage things like asynchronous execution or multithreading to speed it up?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**$\\star \\star \\star \\star \\star$ Implment this asynchronous execution or multithreading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(target_vector, glove_lookup):\n",
    "    \"\"\"\n",
    "    Finds the closest word in the GloVE dictionary to a given vector\n",
    "\n",
    "    Args:\n",
    "        target_vector (np.ndarray): Vector to find the closest word to\n",
    "        glove_lookup (dict): GloVE dictionary to look up the closest word in \n",
    "\n",
    "    Returns:\n",
    "        str: Word in keys of glove_lookup closest to target_vector\n",
    "    \"\"\"\n",
    "    # Define an initial max similarity and closest word (both to be updated)\n",
    "    ...\n",
    "\n",
    "    # Iterate over all words in the GloVE dictionary and find the most similar one to the target vector\n",
    "    ...\n",
    "    \n",
    "\n",
    "    # If the similarity is higher than the current max, update the max similarity and closest word\n",
    "    ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['cat']\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word_vector = glove[word]\n",
    "    closest_word = find_closest_word(word_vector, glove)\n",
    "    print(f\"Closest word found to actual word {words[i]}:\", closest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 4 Comparing word embeddings vectors\n",
    "\n",
    "*Since both GloVe and Fasttext, at their core, are both methods for egenerating embedding vectors, it would make sense to examine how they look compared to one another. Since both live in high-dimensional spaces, we must perform PCA on them to actually make sense of them in a graphical way.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings - Theoretical questions\n",
    "\n",
    "**1. Explain shortly what you expect to find if we perform PCA on the matrix of word-embeddings, that is the matrix which holds a vector representation of each word in our vocabulary**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. When getting vectors for all words in a large text, GloVe should be significantly faster than Fasttext, why is this?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. Which model (GloVe or fasttext) do you think performs best at seperating the four classes of the AG News dataset if we get average embedding vectors for each text in the training and testing dataset, perform PCA on these and plot them on the two top principal components. And why do you think that model performs best at seperating the four classes?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 General for all tasks in exercise 4\n",
    "\n",
    "**1. Below, we define the current_model as a fasttext_word_model. Run the code with this, but also try changing it to a fasttext_char_model or the glove dictionary. All should work, and should produce different results. After rerunning the code cells. Comment generally on the differences observed between the different models**\n",
    "\n",
    "$\\dots$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = glove\n",
    "# current_model_name = fasttext_char_model\n",
    "# current_model = fasttext_word_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = []\n",
    "feats_test = []\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleaning text of non-alphanumerics using the aforementioned string translation table\n",
    "    Just for convenience in regards to GloVe and useful word vectors\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator).lower()\n",
    "\n",
    "def get_average_embedding_vectors(model, train_texts, test_texts):\n",
    "    print(\"Getting fasttext average embedding vectors for each class...\")\n",
    "    placeholder = np.zeros_like(glove['the'])\n",
    "\n",
    "    for text in tqdm(train_texts):\n",
    "        words = clean(text).split()\n",
    "        if getattr(model, 'get', False):\n",
    "            feats_train.append(np.mean([model.get(word, placeholder) for word in words], axis=0))\n",
    "        else:\n",
    "            feats_train.append(np.mean([model.get_word_vector(word) for word in words], axis=0))\n",
    "\n",
    "    # Same but for each text in test set\n",
    "    for text in tqdm(test_texts):\n",
    "        words = clean(text).split()\n",
    "        if getattr(model, 'get', False):\n",
    "            feats_test.append(np.mean([model.get(word, placeholder) for word in words], axis=0))\n",
    "        else:\n",
    "            feats_test.append(np.mean([model.get_word_vector(word) for word in words], axis=0))\n",
    "\n",
    "    return np.array(feats_train), np.array(feats_test)\n",
    "\n",
    "feats_train, feats_test = get_average_embedding_vectors(current_model, train_texts, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(n_components, feats_train, feats_test, train_labels, test_labels, label_names):\n",
    "    \"\"\"\n",
    "    Does PCA with n_components on given features and plots the result in two dimensions\n",
    "\n",
    "    Args:\n",
    "        n_components (int): How many components to use in the PCA\n",
    "        feats_train (np.ndarray): Average embedding vectors for each text in the training set\n",
    "        feats_test (np.ndarray): Average embedding vectors for each text in the test set\n",
    "        train_labels (np.ndarray): Labels for each text in the training set\n",
    "        test_labels (np.ndarray): Labels for each text in the test set\n",
    "        label_names (dict): Dictionary of int: str so that int corresponds to the label name\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # fit_transform avoids having to manually transform the vectors with the matrix afterwards\n",
    "    Vtrain = pca.fit_transform(feats_train)\n",
    "    Vtrain_var_explained = pca.explained_variance_ratio_\n",
    "\n",
    "    Vtest = pca.fit_transform(feats_test)\n",
    "    Vtest_var_explained = pca.explained_variance_ratio_\n",
    "\n",
    "    colors = 'rgbk'\n",
    "    for label, transformed_vector, title in zip([train_labels, test_labels], [Vtrain, Vtest], [f'Training set var_explained: {sum(Vtrain_var_explained[:2])}', f'Test set, var_explained: {sum(Vtest_var_explained[:2])}']):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 0], transformed_vector[label==i, 1], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 1], transformed_vector[label==i, 2], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC2')\n",
    "        plt.ylabel('PC3')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 2], transformed_vector[label==i, 3], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC3')\n",
    "        plt.ylabel('PC4')\n",
    "        plt.suptitle(title, fontweight='bold', fontsize=15)\n",
    "\n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(range(n_components)),np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.title(\"Explained variance compared to number of components\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "    plt.show()\n",
    "    \n",
    "    return pca\n",
    "\n",
    "n_components = 50\n",
    "pca_word_vec = plot_pca(n_components, feats_train, feats_test, train_labels, test_labels, ag_news_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PCA projection\n",
    "\n",
    "**1. Complete the function below to get a word vector a word and project it to a given PCA with n principal components**\n",
    "\n",
    "**2. Comment on the plot created two cells below. Do the vectors correspond to what you'd expect?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. What must we be cognisant of when doing PCA plots like this on $n$ principal components?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_transform(word, model, pca, n=2):\n",
    "    \"\"\"\n",
    "    Gets the PCA transformed vector of a given word in the model with n principal components\n",
    "\n",
    "    Args:\n",
    "        word (str): word to first get the embedding vector of and then project to PCA space\n",
    "        model (dict, fasttext model): Either a fasttext model or GloVe dictionary\n",
    "        pca (sklearn.decomposition.PCA): PCA for the model given, should be the same model as the one used to get the vectors\n",
    "        n (int, optional): Number of princpal components to project to. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Word vector of word projected to PCA space with n principal components\n",
    "    \"\"\"\n",
    "    # Get the word vector from the model (glove or Fasttext)\n",
    "    word_vec = model[word]\n",
    "    # Reshape so it fits with PCA transform\n",
    "    word_vec = word_vec.reshape(1, -1)\n",
    "\n",
    "    # Return the PCA transformed vector with n principal components\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['company', 'business', 'cat', 'software', 'microsoft', 'mouse']\n",
    "\n",
    "transformed_word_vectors = [get_vector_transform(i, current_model, pca_word_vec, n=2) for i in words]\n",
    "plt.figure(figsize=(28, 10))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for idx, vec in enumerate(transformed_word_vectors):\n",
    "    plt.plot([0, vec[0]], [0, vec[1]], 'b--')\n",
    "    circle = plt.Circle((vec[0], vec[1]), radius=0.015)\n",
    "    ax.add_patch(circle)\n",
    "    label = ax.annotate(words[idx], xy=(vec[0], vec[1]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.xlabel('Latent Semantic dim 1')\n",
    "plt.ylabel('Latent Semantic dim 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing with more principal components\n",
    "\n",
    "**1. Run the cell below to compare different words to the previous list of words. How do they compare? Do these comparisons change significantly when you run with more principal components?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "to_compare = ['software', 'business', 'world'] # Three words, that should be labeled as three different things\n",
    "for word in words:\n",
    "    for comparison in to_compare:\n",
    "        distance = word_similarity(\n",
    "                                    vec1=get_vector_transform(word, current_model, pca_word_vec, n=2),\n",
    "                                    vec2=get_vector_transform(comparison, current_model, pca_word_vec,n=2),\n",
    "                                    glove=None\n",
    "                                    )\n",
    "        print(f\"{word}-{comparison}: {distance}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 5 Screwing around with fasttext models\n",
    "\n",
    "*A good thing if we want to use fasttext on a character level, is that it will be able to understand spelling errors. We're going to test this now by replacing a bunch of letters in our test set randomly with other words and once more test the accuracy of the word-wise fasttext vs the character-wise fasttext*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dyslexibot(test_set: list, p=0.05, extra_scuffed=False):\n",
    "    \"\"\"\n",
    "    tHe AlMiGhTy dyslexibot(tm) replaces letters with probability p\n",
    "    extra_scuffed does what it says: it makes the replacements even harder to guess\n",
    "    This function is pretty ineffective, if you want to spruce it up, you are welcome to do so\n",
    "    \"\"\"\n",
    "\n",
    "    if extra_scuffed:\n",
    "        test_set_letters = np.array(list(set(''.join(test_texts)))) # Can replace with all letters currently in test set\n",
    "    else:\n",
    "        test_set_letters = np.array(list(string.ascii_lowercase)) # Can only replace with lowercase letters\n",
    "\n",
    "    new_test_set = [text.split(' ') for text in test_set.copy()]\n",
    "\n",
    "    print(\"Replacing text\")\n",
    "    for i, text in tqdm(enumerate(new_test_set)):\n",
    "        for r, word in enumerate(text):\n",
    "            word = list(word)\n",
    "            for t, letter in enumerate(word):\n",
    "                rand = random.uniform(0, 1)\n",
    "\n",
    "                if extra_scuffed and rand < p: # We replace even spaces!\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "                elif letter != ' ' and rand < p:\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "            text[r] = ''.join(word)\n",
    "        new_test_set[i] = ' '.join(text)\n",
    "    return np.array(new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing that dyslexibot works\n",
    "text = [\"have you heard of the tragedy of darth plagueis the wise\"]\n",
    "print(dyslexibot(text, p=0.10, extra_scuffed=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Dyslexibot and fasttext\n",
    "\n",
    "**1. Test both the fasttext_word_model and the fasttext_char_model on text generated by dyslexibot. Change the $p$ value and perhaps the $\\text{extra scuffed}$ option. Try to make the fasttext_word_model as bad as possible while the fasttext_char_model still keeps somewhat good performance. Comment on what you did to achieve this.**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dyslexitext = dyslexibot(test_texts, p=0.20, extra_scuffed=False)\n",
    "\n",
    "# Insert your training loop here to calculate the test accuracy on the dyslexitext\n",
    "# using both the word-wise fasttext and the character-wise fasttext\n",
    "\n",
    "\n",
    "print(\"Testing fasttext word model...\")\n",
    "test_fasttext_model(fasttext_word_model, dyslexitext, test_labels, ag_news_labels)\n",
    "\n",
    "print(\"\\nTesting fasttext character model...\")\n",
    "test_fasttext_model(fasttext_char_model, dyslexitext, test_labels, ag_news_labels)\n",
    "print()\n",
    "\n",
    "\n",
    "# # TODO: What it says one line above\n",
    "# ???????????\n",
    "# ???????????\n",
    "# ???????????\n",
    "# raise NotImplementedError(\"Test me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Custom text classification\n",
    "\n",
    "**1. Finally, to show the robustness of fasttext (and for fun), you are to make your own text and try to let fasttext classify this as one of the four models. Rememeber that the original dataset used texts of around 240 words, so you can either experiment with texts shorter or longer than this and see whether the accuracy is significantly different.**\n",
    "\n",
    "**As an extra challenge, you can try to create a text which is as close to the model's decision boundary as possible, IE. one that is as close as possible to being classified as either two or more of the classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_text = \"\"\n",
    "\n",
    "assert own_text != \"\", \"Come on, be creative\"\n",
    "\n",
    "print(\"The text is classified as: \", fasttext_word_model.predict(own_text))#[0][0][9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\star$ $\\star$ $\\star$ Exercise 6 Extremely optional task\n",
    "\n",
    "**1. It's quite a waste to have all those juicy principle components of the embedding matrices without using them in a classifier of some sort, right?**\n",
    "\n",
    "**Create a classifier to classify texts as one of the four labels, based on the projections of their words unto a number of the principle components of the embedding matrices. Compare this classifier to the fasttext classifier and reflect on their performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Asking for a whole classifier is a bit much, no?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02462",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
